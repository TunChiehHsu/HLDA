\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}


\title{Hierarchical Topic Models and
the Nested Chinese Restaurant Process}
\author{Tun-Chieh Hsu, Xialingzi Jin,Yen-Hua Chen}

\begin{document}
\maketitle



\section{Abstract}


\section{Background}

The paper we selected is Hierarchical Topic Models and
the Nested Chinese Restaurant Process. 
Recently, complex probabilistic models are increasingly prevalent in various of  domains. However, there are several challenges that should be dealt with due to there open-ended nature, that is, the data sets often grow over time, and as they grow they bring new entities and new structures to the fore. Take the problem of learning a topic hierarchy from data for example. Given a collection of "documents", each of which contains a
set of "words" and the goal is to discover common usage patterns or "topics" in the documents,
and to organize these topics into a hierarchy. 


This paper propose a new method that specified a generative probabilistic model
for hierarchical structures and adopt Bayesian perspective to learn such structures from data. 
The hierarchies in this case could be considered as random variables and specified procedurally.
In addition, the underlying approach of constructing the probabilistic object is Chinese restaurant process (CRP), a distribution on partitions of integers. In this paper, they extend CRP to a hierarchy of partitions and apply it as a representation of prior and posterior distributions for topic hierarchies. To be more specific, each node in the hierarchy is associated with a topic, where a topic is a distribution across
words. A document is generated by choosing a path from the root to a leaf, repeatedly
sampling topics along that path, and sampling the words from the selected topics. Thus
the organization of topics into a hierarchy aims to capture the breadth of usage of topics
across the corpus, reflecting underlying syntactic and semantic notions of generality and
specificity. 

There are several approaches to the modeling of topic hierarchies in the literatures. Most of them were constructed on the hypothesis that the distributions between leaf and root are similar. This approach, however, does not based on such constraint. ..... 

\section{Algorithm Description}
In order to introduce this method, we should first introduce the Chinese restaurant process (CRP). 
\subsection{The Chinese restaurant process}

CRP is an analogous to seating customers at tables in a Chinese restaurant. Imagine there is a Chinese restaurant with an infinite number of circular tables, each with infinite capacity. Customer 1 sits at the first table. The next customer either sits at the same table as customer 1, or the next table.  The $mth$ subsequent customer sits at a table drawn from the following
distribution: 
\begin{align}
p(\text{occupied table i| previous customers}) =  \frac{m_i}{\gamma+m-1}  \\ 
p(\text{next unoccupied table | previous customers}) = \frac{\gamma}{\gamma + m -1} &
\end{align}
where $m_i$ is the number of previous customers at table $i$, and $\gamma$ is a parameter. After $M$
customers sit down, the seating plan gives a partition of $M$ items. This distribution gives
the same partition structure as draws from a Dirichlet process.

\subsection{Nested Chinese restaurant process}

A nested Chinese restaurant process (nCRP) is an extended version of CRP. Suppose that there are an infinite number of infinite-table Chinese restaurants in a city. And restaurant is determined to be the root restaurant and on each of its infinite tables is a card
with the name of another restaurant. On each of the tables in those restaurants are cards that
refer to other restaurants, and this structure repeats infinitely. Each restaurant is referred to
exactly once. As a result, the whole process could be imagined as an infinitely-branched tree. Now, consider 
a tourist arrives in the city for a culinary vacation. On the first first day, he select a root
Chinese restaurant and selects a table from the equation above. On the second day, he enters to the
restaurant refered by previous restaurant , again from the above equation.
This process was repeated for $L$ days, and at the end, the tourist has sat at L restaurants
which constitute a path from the root to a restaurant at the $Lth$ level in the infinite tree
After M tourists take L-day vacations, the collection of paths describe a
particular L-level subtree of the infinite tree.

\subsection{A hierarchical topic model}

\begin{enumerate}
\item Let $c_1$ be the root restaurant
\item For each level $l \in \{2, . . . , L\}$: \\
	(a) Draw a table from restaurant $c_{l?1}$ using Eq. (1) and (2). Set $c_l$ to be the restaurant
referred to by that table.
\item  Draw an L-dimensional topic proportion vector $\theta$ from $Dir(\alpha)$.
\item For each word $n \in {1, . . . , N}$:\\
(a) Draw $z \in {1, . . . , L}$ from $Mult(\theta)$\\
(b) Draw $w_n$ from the topic associated with restaurant $c_z$.

\end{enumerate}






\bibliographystyle{alpha}
\bibliography{663_Final}

\end{document}