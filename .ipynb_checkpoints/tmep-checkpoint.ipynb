{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Sample Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nltk.tokenize",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7c55d655fdae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named nltk.tokenize"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gamma\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RegexpTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a7be45175137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# create English stop words list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0men_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RegexpTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# create sample documents\n",
    "doc_a = \"Batman became popular soon after his introduction and gained his own comic book title, Batman, in 1940.\"\n",
    "\n",
    "doc_b = \"In 1971, Trump moved to Manhattan, where he became involved in larger construction projects, and used attractive architectural design to win public recognition.\"\n",
    "\n",
    "doc_c = \"Batman is, in his everyday identity, Bruce Wayne, a wealthy American business magnate living in Gotham City.\"\n",
    "\n",
    "doc_d = \"In 2001, Trump completed Trump World Tower, a 72-story residential tower across from the United Nations Headquarters.\"\n",
    "\n",
    "doc_e = \" Unlike most superheroes, Batman does not possess any superpowers; rather, he relies on his genius intellect, physical prowess, martial arts abilities, detective skills, science and technology, vast wealth, intimidation, and indomitable will. \"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## CRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def CRP(topic, gamma):\n",
    "    '''CRP gives the probability of topic assignment for specific vocabulary'''\n",
    "    '''Return a j * 1 vector, where j is the number of topic'''\n",
    "    cm = []\n",
    "    m = sum([len(x) for x in topic])\n",
    "    p = gamma / (gamma + m) # prob for new topic\n",
    "    cm.append(p)\n",
    "    for j in range(len(topic)):\n",
    "        p = len(topic[j]) / (gamma + m) # prob for existing topics\n",
    "        cm.append(p)\n",
    "    return np.array(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## node sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def node_sampling(corpus_s, gamma):\n",
    "    '''Node sampling samples the number of topics for next level'''\n",
    "    topic = []    \n",
    "    for corpus in corpus_s:\n",
    "        for doc in corpus:\n",
    "            cm = CRP(topic, gamma)\n",
    "            theta = np.random.multinomial(1, (cm/sum(cm))).argmax()\n",
    "            if theta == 0:\n",
    "                # create new topic\n",
    "                topic.append([doc])\n",
    "            else:\n",
    "                # existing topic\n",
    "                topic[theta-1].append(doc)\n",
    "    return topic    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Z\n",
    "$P(z_{i}=j\\hspace{1ex}|\\hspace{1ex}{\\bf z}_{-i},{\\bf w})\\propto\\frac{n_{-i,j}^{(w_{i})}+\\beta}{n_{-i,j}^{(\\cdot)}+W\\beta}\\frac{n_{-i,j}^{(d_{i})}+\\alpha}{n_{-i,\\cdot}^{(d_{i})}+T\\alpha}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def Z(corpus_s, topic, alpha, beta):\n",
    "    '''Z distributes each vocabulary to topics'''\n",
    "    '''Return a n * 1 vector, where n is the number of vocabularies'''\n",
    "    n_vocab = sum([len(x) for x in corpus_s])\n",
    "    # zm: n * 1\n",
    "    # return the assignment of each vocabulary\n",
    "    t_zm = np.zeros(n_vocab).astype('int')\n",
    "    # z_assigned: j * 1\n",
    "    # return a list of list topic where stores assigned vocabularies in each sublist\n",
    "    z_assigned = [[] for _ in topic]\n",
    "    z_doc = [[] for _ in topic]\n",
    "    z_tmp = np.zeros((n_vocab, len(topic)))\n",
    "    assigned = np.zeros((len(corpus_s), len(topic)))\n",
    "    n = 0\n",
    "    for i in range(len(corpus_s)):\n",
    "        for d in range(len(corpus_s[i])): \n",
    "            wi = corpus_s[i][d]   \n",
    "            for j in range(len(topic)):\n",
    "                lik = (z_assigned[j].count(wi) + beta) / (assigned[i, j] + n_vocab * beta)\n",
    "                pri = (len(z_assigned[j]) + alpha) / ((len(corpus_s[i]) - 1) + len(topic) * alpha)\n",
    "                z_tmp[n, j] = lik * pri\n",
    "                t_zm[n] = np.random.multinomial(1, (z_tmp[n,:]/sum(z_tmp[n,:]))).argmax()\n",
    "            z_assigned[t_zm[n]].append(wi)\n",
    "            z_doc[t_zm[n]].append(i)\n",
    "            assigned[i, t_zm[n]] += 1\n",
    "            n += 1\n",
    "    z_assigned = [x for x in z_assigned if x != []]\n",
    "    z_doc = [x for x in z_doc if x != []]\n",
    "    return np.array(z_assigned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def C(corpus_s, topic, gamma):\n",
    "    cm = []\n",
    "    for corpus in corpus_s:\n",
    "        for word in corpus:\n",
    "            for t in topic:\n",
    "                if type(t) == list:\n",
    "                    y = t.count(word)\n",
    "                else:\n",
    "                    y = t.tolist().count(word)\n",
    "        H = np.random.poisson(lam=(2), size=(len(topic)))\n",
    "        alpha = gamma*H\n",
    "        temp = np.random.dirichlet(y + alpha).transpose()    \n",
    "        cm.append((temp/sum(temp)).tolist())\n",
    "    return np.array(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "most_common = lambda x: Counter(x).most_common(1)[0][0]\n",
    "def wn(c_m, corpus_s, topic):\n",
    "    wn_topic = []\n",
    "    for i, corpus in enumerate(corpus_s):\n",
    "        for word in corpus:\n",
    "            theta = np.random.multinomial(1, c_m[i]).argmax()\n",
    "            wn_topic.append(theta)\n",
    "    return np.array(wn_topic)\n",
    "\n",
    "            \n",
    "def gibbs_wn(c_m, corpus_s, topic, ite):\n",
    "    n_vocab = sum([len(x) for x in corpus_s])\n",
    "    wn_gibbs = np.empty((n_vocab, ite)).astype('int')\n",
    "    for i in range(ite):\n",
    "        wn_gibbs[:, i] = wn(c_m, corpus_s, topic)\n",
    "    # drop first 1/10 data\n",
    "    wn_gibbs = wn_gibbs[:, int(ite/10):]\n",
    "    theta = [most_common(wn_gibbs[x]) for x in range(n_vocab)]\n",
    "    \n",
    "    wn_topic = [[] for _ in topic]\n",
    "    wn_doc_topic = [[] for _ in topic]\n",
    "    doc = 0\n",
    "    n = 0\n",
    "    for i, corpus_s in enumerate(corpus_s):\n",
    "        if doc == i:\n",
    "            for word in corpus_s:\n",
    "                wn_doc_topic[theta[n]].append(word)\n",
    "                n += 1\n",
    "            for j in range(len(topic)):\n",
    "                if wn_doc_topic[j] != []:\n",
    "                    wn_topic[j].append(wn_doc_topic[j])\n",
    "        wn_doc_topic = [[] for _ in topic]        \n",
    "        doc += 1\n",
    "    wn_topic = [x for x in wn_topic if x != []]\n",
    "    return wn_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## hLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def hLDA(corpus_s, gamma, alpha, beta, ite, level):\n",
    "    \n",
    "    # 1. Node sampling, samples max level L\n",
    "    topic = node_sampling(corpus_s, gamma)\n",
    "    \n",
    "    def dis(corpus_s, gamma, alpha, beta, ite):\n",
    "        \n",
    "        # 2. z_m, samples topic from L\n",
    "        z_topic = Z(corpus_s, topic, alpha, beta)\n",
    "        \n",
    "        # 3. c_m, samples path\n",
    "        c_m = C(corpus_s, z_topic, gamma)\n",
    "        \n",
    "        # 4. w_n, distributes words into topics\n",
    "        wn_topic = gibbs_wn(c_m, corpus_s, z_topic, ite)\n",
    "\n",
    "        return wn_topic\n",
    "    \n",
    "    hLDA_tree = [[] for _ in range(level)]\n",
    "    tmp_tree = []\n",
    "    node = [[] for _ in range(level+1)]\n",
    "    node[0].append(1)\n",
    "    \n",
    "    for i in range(level):\n",
    "        if i == 0:\n",
    "            wn_topic = dis(texts, gamma, alpha, beta, ite)\n",
    "            topic = set([x for list in wn_topic[0] for x in list])\n",
    "            hLDA_tree[0].append(topic)\n",
    "            tmp_tree.append(wn_topic[1:])\n",
    "            tmp_tree = tmp_tree[0]\n",
    "            node[1].append(len(wn_topic[1:]))\n",
    "        else:\n",
    "            for j in range(sum(node[i])):\n",
    "                if tmp_tree == []:\n",
    "                    break\n",
    "                wn_topic = dis(tmp_tree[0], gamma, alpha, beta, ite)\n",
    "                topic = set([x for list in wn_topic[0] for x in list])\n",
    "                hLDA_tree[i].append(topic)\n",
    "                tmp_tree.remove(tmp_tree[0])\n",
    "                if wn_topic[1:] != []:\n",
    "                    tmp_tree.extend(wn_topic[1:])\n",
    "                node[i+1].append(len(wn_topic[1:]))\n",
    "        \n",
    "    return hLDA_tree, node[:level]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [['batman',\n",
    "  'becam',\n",
    "  'popular',\n",
    "  'soon',\n",
    "  'introduct',\n",
    "  'gain',\n",
    "  'comic',\n",
    "  'book',\n",
    "  'titl',\n",
    "  'batman',\n",
    "  '1940'],\n",
    " ['1971',\n",
    "  'trump',\n",
    "  'move',\n",
    "  'manhattan',\n",
    "  'becam',\n",
    "  'involv',\n",
    "  'larger',\n",
    "  'construct',\n",
    "  'project',\n",
    "  'use',\n",
    "  'attract',\n",
    "  'architectur',\n",
    "  'design',\n",
    "  'win',\n",
    "  'public',\n",
    "  'recognit'],\n",
    " ['batman',\n",
    "  'everyday',\n",
    "  'ident',\n",
    "  'bruce',\n",
    "  'wayn',\n",
    "  'wealthi',\n",
    "  'american',\n",
    "  'busi',\n",
    "  'magnat',\n",
    "  'live',\n",
    "  'gotham',\n",
    "  'citi'],\n",
    " ['2001',\n",
    "  'trump',\n",
    "  'complet',\n",
    "  'trump',\n",
    "  'world',\n",
    "  'tower',\n",
    "  '72',\n",
    "  'stori',\n",
    "  'residenti',\n",
    "  'tower',\n",
    "  'across',\n",
    "  'unit',\n",
    "  'nation',\n",
    "  'headquart'],\n",
    " ['unlik',\n",
    "  'superhero',\n",
    "  'batman',\n",
    "  'possess',\n",
    "  'superpow',\n",
    "  'rather',\n",
    "  'reli',\n",
    "  'geniu',\n",
    "  'intellect',\n",
    "  'physic',\n",
    "  'prowess',\n",
    "  'martial',\n",
    "  'art',\n",
    "  'abil',\n",
    "  'detect',\n",
    "  'skill',\n",
    "  'scienc',\n",
    "  'technolog',\n",
    "  'vast',\n",
    "  'wealth',\n",
    "  'intimid',\n",
    "  'indomit',\n",
    "  'will']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Pineapple.app/Contents/Resources/python2.7/lib/python2.7/site-packages/ipykernel/__main__.py:7: RuntimeWarning: divide by zero encountered in divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[{'1940',\n",
       "    '1971',\n",
       "    '2001',\n",
       "    '72',\n",
       "    'abil',\n",
       "    'across',\n",
       "    'american',\n",
       "    'architectur',\n",
       "    'art',\n",
       "    'attract',\n",
       "    'batman',\n",
       "    'becam',\n",
       "    'book',\n",
       "    'bruce',\n",
       "    'busi',\n",
       "    'citi',\n",
       "    'comic',\n",
       "    'complet',\n",
       "    'construct',\n",
       "    'design',\n",
       "    'detect',\n",
       "    'everyday',\n",
       "    'gain',\n",
       "    'geniu',\n",
       "    'gotham',\n",
       "    'headquart',\n",
       "    'ident',\n",
       "    'indomit',\n",
       "    'intellect',\n",
       "    'intimid',\n",
       "    'introduct',\n",
       "    'involv',\n",
       "    'larger',\n",
       "    'live',\n",
       "    'magnat',\n",
       "    'manhattan',\n",
       "    'martial',\n",
       "    'move',\n",
       "    'nation',\n",
       "    'physic',\n",
       "    'popular',\n",
       "    'possess',\n",
       "    'project',\n",
       "    'prowess',\n",
       "    'public',\n",
       "    'rather',\n",
       "    'recognit',\n",
       "    'reli',\n",
       "    'residenti',\n",
       "    'scienc',\n",
       "    'skill',\n",
       "    'soon',\n",
       "    'stori',\n",
       "    'superhero',\n",
       "    'superpow',\n",
       "    'technolog',\n",
       "    'titl',\n",
       "    'tower',\n",
       "    'trump',\n",
       "    'unit',\n",
       "    'unlik',\n",
       "    'use',\n",
       "    'vast',\n",
       "    'wayn',\n",
       "    'wealth',\n",
       "    'wealthi',\n",
       "    'will',\n",
       "    'win',\n",
       "    'world'}],\n",
       "  [],\n",
       "  [],\n",
       "  []],\n",
       " [[1], [0], [], []])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hLDA(texts, 10, 0.1, 0.01, 10000, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-a9ce39a5d074>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-36-a9ce39a5d074>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    sudo -H pip3 install ete2\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
