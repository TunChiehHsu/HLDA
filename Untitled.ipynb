{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def temp(corpus_s, topic,eta):\n",
    "    result = []\n",
    "    allword_topic = [word  for t in topic for word in t]\n",
    "    n_vocab = sum([len(x) for x in corpus_s])\n",
    "    for corpus in corpus_s:\n",
    "        prob_result = []\n",
    "        for j in range(len(topic)):\n",
    "            current_topic = topic[j]\n",
    "            n_word_topic = len(current_topic)\n",
    "            prev_dominator = 1\n",
    "            later_numerator = 1\n",
    "            prob_word = 1  \n",
    "\n",
    "            \n",
    "            overlap = list(set(corpus) & set(current_topic))\n",
    "            for word in corpus:\n",
    "                #print(len(current_topic) - len(overlap))\n",
    "                prev_numerator = gamma(len(current_topic) - len(overlap) + n_vocab * eta)\n",
    "                #print(prev_numerator)\n",
    "                later_dominator = gamma(len(current_topic) + n_vocab * eta)\n",
    "                #print(later_dominator)\n",
    "                corpus_list = corpus.tolist()\n",
    "                \n",
    "                if current_topic.count(word) - corpus_list.count(word) < 0 :\n",
    "                    a = 0\n",
    "                else:\n",
    "                    a = current_topic.count(word) - corpus_list.count(word)\n",
    "                prev_dominator *= gamma(a + eta)\n",
    "                #print(prev_dominator)\n",
    "                #print(prev_numerator)\n",
    "                later_numerator *= gamma(current_topic.count(word) + eta) \n",
    "                #print(prev_numerator/prev_dominator , later_numerator/later_dominator,prev_numerator/prev_dominator * later_numerator/later_dominator)\n",
    "            prev = prev_numerator/prev_dominator\n",
    "            later = later_numerator/later_dominator\n",
    "            \n",
    "            like = prev * later \n",
    "            #print(prev,later,like)\n",
    "            prob_word = prob_word * like\n",
    "                #prob_word *= (prev_numerator/prev_dominator * later_numerator/later_dominator)\n",
    "            prob_result.append(prob_word)\n",
    "        result.append(prob_result)\n",
    "        \n",
    "    a = np.array(result)    \n",
    "    b = np.prod(a,axis = 1)\n",
    "    return(b)             \n",
    "                \n",
    "\n",
    "            \n",
    "p(zd,n = k | zd,−n)         \n",
    "   \n",
    "def Z(corpus_s, topic, alpha, beta):\n",
    "    '''Z distributes each vocabulary to topics'''\n",
    "    '''Return a n * 1 vector, where n is the number of vocabularies'''\n",
    "    n_vocab = sum([len(x) for x in corpus_s])\n",
    "    # zm: n * 1\n",
    "    # return the assignment of each vocabulary\n",
    "    t_zm = np.zeros(n_vocab).astype('int')\n",
    "    # z_assigned: j * 1\n",
    "    # return a list of list topic where stores assigned vocabularies in each sublist\n",
    "    z_assigned = [[] for _ in topic]\n",
    "    z_doc = [[] for _ in topic]\n",
    "    z_tmp = np.zeros((n_vocab, len(topic)))\n",
    "    assigned = np.zeros((len(corpus_s), len(topic)))\n",
    "    n = 0\n",
    "    for i in range(len(corpus_s)):\n",
    "        for d in range(len(corpus_s[i])): \n",
    "            wi = corpus_s[i][d]   \n",
    "            for j in range(len(topic)):\n",
    "                lik = (z_assigned[j].count(wi) + beta) / (assigned[i, j] + n_vocab * beta)\n",
    "                pri = (len(z_assigned[j]) + alpha) / ((len(corpus_s[i]) - 1) + len(topic) * alpha)\n",
    "                z_tmp[n, j] = lik * pri\n",
    "                t_zm[n] = np.random.multinomial(1, (z_tmp[n,:]/sum(z_tmp[n,:]))).argmax()\n",
    "            z_assigned[t_zm[n]].append(wi)\n",
    "            z_doc[t_zm[n]].append(i)\n",
    "            assigned[i, t_zm[n]] += 1\n",
    "            n += 1\n",
    "    z_assigned = [x for x in z_assigned if x != []]\n",
    "    z_doc = [x for x in z_doc if x != []]\n",
    "    return np.array(z_assigned), np.array(z_doc)\n",
    "    \n",
    "CRP    \n",
    "def CRP(topic, gamma):\n",
    "    '''CRP gives the probability of topic assignment for specific vocabulary'''\n",
    "    '''Return a j * 1 vector, where j is the number of topic'''\n",
    "    cm = []\n",
    "    m = sum([len(x) for x in topic])\n",
    "    p = gamma / (gamma + m) # prob for new topic\n",
    "    cm.append(p)\n",
    "    for j in range(len(topic)):\n",
    "        p = len(topic[j]) / (gamma + m) # prob for existing topics\n",
    "        cm.append(p)\n",
    "    return np.array(cm)\n",
    "\n",
    "Node Sampling\n",
    "def node_sampling(corpus_s, gamma):\n",
    "    '''Node sampling samples the number of topics for next level'''\n",
    "    topic = []    \n",
    "    for corpus in corpus_s:\n",
    "        for doc in corpus:\n",
    "            cm = CRP(topic, gamma)\n",
    "            theta = np.random.multinomial(1, (cm/sum(cm))).argmax()\n",
    "            if theta == 0:\n",
    "                # create new topic\n",
    "                topic.append([doc])\n",
    "            else:\n",
    "                # existing topic\n",
    "                topic[theta-1].append(doc)\n",
    "    return topic    \n",
    "    \n",
    "\n",
    "       \n",
    "def CRP_prior(corpus_s, doc, gamma):\n",
    "    m_topic = []\n",
    "    cm = []\n",
    "    for i in range(len(corpus_s)):\n",
    "        for j in range(len(doc)):\n",
    "            m_topic.append(np.setdiff1d(doc[j], i).tolist())\n",
    "        for m in range(len(corpus_s[i])):\n",
    "            tmp = CRP(m_topic, gamma)\n",
    "            theta = np.random.multinomial(1, (tmp/sum(tmp))).argmax()\n",
    "            if theta == 0:\n",
    "                m_topic.append([corpus_s[i][m]])\n",
    "            else:\n",
    "                m_topic[theta-1].append(corpus_s[i][m])\n",
    "            cm.append(tmp.tolist())\n",
    "        m_topic = []\n",
    "    return np.array(cm)\n",
    "    \n",
    "    def nCRP(corpus_s, wm, cm):\n",
    "    n = 0\n",
    "    n_cm = []\n",
    "    for i in range(len(corpus_s)):\n",
    "        for j in range(len(corpus_s[i])):\n",
    "            tmp = wm[i] * np.array(cm[n])\n",
    "            n_cm.append((tmp/sum(tmp)).tolist())\n",
    "            n += 1\n",
    "    return np.array(n_cm)\n",
    "       \n",
    "       \n",
    "    for corpus in corpus_s:\n",
    "        for word in corpus:\n",
    "            y = topic.Count(words)\n",
    "        H = np.random.poisson(lam=(2), size=(n_vocab))\n",
    "        alpha = gamma*H\n",
    "        temp = np.random.dirichlet(y+alpha, len(topic)).transpose().tolist()\n",
    "        wm.append(temp)\n",
    "    return np.array(wm)\n",
    "        \n",
    "n_vocab = 100\n",
    "gamma = 10\n",
    "wm = []\n",
    "for corpus in corpus_s:\n",
    "        for word in corpus:\n",
    "            for t in topic:\n",
    "                y = t.count(word)\n",
    "                H = np.random.poisson(lam=(2), size=(len(topic)))\n",
    "                alpha = gamma*H\n",
    "                temp = np.random.dirichlet(y + alpha).transpose().tolist()    \n",
    "      return wm.append(temp)\n",
    "        \n",
    "        \n",
    "        \n",
    "wM = np.array(wm)\n",
    "        \n",
    "def cm(corpus_s, topic, gamma):\n",
    "    wm = []\n",
    "    for corpus in corpus_s:\n",
    "        for word in corpus:\n",
    "            for t in topic:\n",
    "                    y = t.count(word)\n",
    "        H = np.random.poisson(lam=(2), size=(len(topic)))\n",
    "        alpha = gamma*H\n",
    "        temp = np.random.dirichlet(y + alpha).transpose()    \n",
    "        wm.append((temp/sum(temp)).tolist())\n",
    "    return np.array(wm)      \n",
    "        \n",
    "        \n",
    "def wn(c_m, corpus_s, topic):\n",
    "    a_topic = [[] for _ in topic]\n",
    "    for i, corpus in enumerate(corpus_s):\n",
    "        for word in corpus:\n",
    "            theta = np.random.multinomial(1, c_m[i]).argmax()\n",
    "            a_topic[theta].append(word)\n",
    "    return a_topic\n",
    "\n",
    "\n",
    "def hLDA(corpus_s, gamma, alpha, beta, ite, level):\n",
    "    \n",
    "    # 1. Node sampling, samples max level L\n",
    "    topic = node_sampling(corpus_s, gamma)\n",
    "    \n",
    "    def dis(corpus_s, gamma, alpha, beta, ite):\n",
    "        \n",
    "        # 2. z_m, samples topic from L\n",
    "        z_topic = Z(corpus_s, topic, alpha, beta)\n",
    "        \n",
    "        # 3. c_m, samples path\n",
    "        c_m = C(corpus_s, z_topic, gamma)\n",
    "        \n",
    "        # 4. w_n, distributes words into topics\n",
    "        wn_topic = gibbs_wn(c_m, corpus_s, z_topic, ite)\n",
    "\n",
    "        return wn_topic\n",
    "    \n",
    "    hLDA_tree = [[] for _ in range(level)]\n",
    "    tmp_tree = []\n",
    "    node = [[] for _ in range(level+1)]\n",
    "    node[0].append(1)\n",
    "    \n",
    "    for i in range(level):\n",
    "        if i == 0:\n",
    "            wn_topic = dis(texts, gamma, alpha, beta, ite)\n",
    "            hLDA_tree[0].append(wn_topic[0])\n",
    "            tmp_tree.append(wn_topic[1:])\n",
    "            tmp_tree = tmp_tree[0]\n",
    "            node[1].append(len(wn_topic[1:]))\n",
    "        else:\n",
    "            for j in range(sum(node[i])):\n",
    "                if tmp_tree == []:\n",
    "                    break\n",
    "                wn_topic = dis(tmp_tree[0], gamma, alpha, beta, ite)\n",
    "                hLDA_tree[i].append(wn_topic[0])\n",
    "                tmp_tree.remove(tmp_tree[0])\n",
    "                if wn_topic[1:] != []:\n",
    "                    tmp_tree.extend(wn_topic[1:])\n",
    "                node[i+1].append(len(wn_topic[1:]))\n",
    "        \n",
    "    return hLDA_tree, node\n",
    "    \n",
    "    hLDA(texts, 5, 0.1, 0.01, 20, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-2.0.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (5.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.6MB 137kB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.7.0 in /usr/local/lib/python2.7/site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python2.7/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.3 in /usr/local/lib/python2.7/site-packages (from gensim)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.5.2.tar.gz\n",
      "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading boto-2.46.1-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 588kB/s \n",
      "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in /usr/local/lib/python2.7/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/mueric35/Library/Caches/pip/wheels/02/44/43/68e963ce2b45baefa913a4e558bcd787403458afddffcf45ca\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/mueric35/Library/Caches/pip/wheels/31/9c/20/996d65ca104cbca940b1b053299b68459391c01c774d073126\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: boto, bz2file, smart-open, gensim\n",
      "Successfully installed boto-2.46.1 bz2file-0.98 gensim-2.0.0 smart-open-1.5.2\n",
      "Collecting stop_words\n",
      "  Downloading stop-words-2015.2.23.1.tar.gz\n",
      "Building wheels for collected packages: stop-words\n",
      "  Running setup.py bdist_wheel for stop-words ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/mueric35/Library/Caches/pip/wheels/22/74/80/77275c2f9f2f1d9841b51e169a38985640a10fbd2711d10791\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2015.2.23.1\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python2.7/site-packages\n",
      "Collecting genism\n",
      "\u001b[31m  Could not find a version that satisfies the requirement genism (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for genism\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install stop_words\n",
    "!pip install nltk\n",
    "!pip install genism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named stop_words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b0e42389dddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named stop_words"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# create sample documents\n",
    "doc_a = \"Batman became popular soon after his introduction and gained his own comic book title, Batman, in 1940.\"\n",
    "\n",
    "doc_b = \"In 1971, Trump moved to Manhattan, where he became involved in larger construction projects, and used attractive architectural design to win public recognition.\"\n",
    "\n",
    "doc_c = \"Batman is, in his everyday identity, Bruce Wayne, a wealthy American business magnate living in Gotham City.\"\n",
    "\n",
    "doc_d = \"In 2001, Trump completed Trump World Tower, a 72-story residential tower across from the United Nations Headquarters.\"\n",
    "\n",
    "doc_e = \" Unlike most superheroes, Batman does not possess any superpowers; rather, he relies on his genius intellect, physical prowess, martial arts abilities, detective skills, science and technology, vast wealth, intimidation, and indomitable will. \"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-41491eef9f0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhLDA_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mhLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "def hLDA(corpus_s, gamma, alpha, beta, ite, level):\n",
    "    \n",
    "    # 1. Node sampling, samples max level L\n",
    "    topic = node_sampling(corpus_s, gamma)\n",
    "    \n",
    "    def dis(corpus_s, gamma, alpha, beta, ite):\n",
    "        \n",
    "        # 2. z_m, samples topic from L\n",
    "        z_topic = Z(corpus_s, topic, alpha, beta)\n",
    "        \n",
    "        # 3. c_m, samples path\n",
    "        c_m = C(corpus_s, z_topic, gamma)\n",
    "        \n",
    "        # 4. w_n, distributes words into topics\n",
    "        wn_topic = gibbs_wn(c_m, corpus_s, z_topic, ite)\n",
    "\n",
    "        return wn_topic\n",
    "    \n",
    "    hLDA_tree = [[] for _ in range(level)]\n",
    "    tmp_tree = []\n",
    "    node = [[] for _ in range(level+1)]\n",
    "    node[0].append(1)\n",
    "    \n",
    "    for i in range(level):\n",
    "        if i == 0:\n",
    "            wn_topic = dis(texts, gamma, alpha, beta, ite)\n",
    "            hLDA_tree[0].append(wn_topic[0])\n",
    "            tmp_tree.append(wn_topic[1:])\n",
    "            tmp_tree = tmp_tree[0]\n",
    "            node[1].append(len(wn_topic[1:]))\n",
    "        else:\n",
    "            for j in range(sum(node[i])):\n",
    "                if tmp_tree == []:\n",
    "                    break\n",
    "                wn_topic = dis(tmp_tree[0], gamma, alpha, beta, ite)\n",
    "                hLDA_tree[i].append(wn_topic[0])\n",
    "                tmp_tree.remove(tmp_tree[0])\n",
    "                if wn_topic[1:] != []:\n",
    "                    tmp_tree.extend(wn_topic[1:])\n",
    "                node[i+1].append(len(wn_topic[1:]))\n",
    "        \n",
    "    return hLDA_tree, node\n",
    "    \n",
    "hLDA(texts, 5, 0.1, 0.01, 20, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
